{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chest_Opacity.ipynb",
      "provenance": [],
      "mount_file_id": "1eU2D0-7cjTrOHjOJQvccnAPtvuHOk77U",
      "authorship_tag": "ABX9TyNI6tno9EkSXnaiPAez2ubo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ms-Ainebyona/Chest-Opacity-project/blob/colab-main/Chest_Opacity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "phwSuoearn02"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import keras.utils\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, optimizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "import os\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from tensorflow.keras import layers\n",
        "img_height=488\n",
        "img_width=395\n",
        "batch_size=2\n",
        "\n",
        "\n",
        "model=keras.Sequential([\n",
        "    layers.Input((28,28,1)),\n",
        "    layers.Conv2D(16,3, padding='same'),\n",
        "    layers.Conv2D(32,3, padding='same'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(2),\n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "-HnZrWNVr7ll"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir=\"/content/drive/MyDrive/Deep learning/Dataset1\""
      ],
      "metadata": {
        "id": "vmrIHersg5L6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train=tf.keras.preprocessing.image_dataset_from_directory(data_dir,\n",
        "labels='inferred', \n",
        "label_mode=\"int\", \n",
        "color_mode='grayscale',\n",
        "batch_size=batch_size,\n",
        " image_size = (img_height,img_width),\n",
        " shuffle=True,\n",
        " seed=123, \n",
        " validation_split=0.1,\n",
        " subset=\"training\",                                                            \n",
        ")\n",
        "ds_validation=tf.keras.preprocessing.image_dataset_from_directory(data_dir, \n",
        "labels='inferred', \n",
        "label_mode=\"int\", \n",
        "color_mode='grayscale',\n",
        "batch_size=batch_size,\n",
        " image_size = (img_height,img_width),\n",
        " shuffle=True,\n",
        " seed=123,\n",
        " validation_split=0.1,\n",
        " subset=\"validation\", \n",
        ")\n",
        "def augment(X,Y):\n",
        "  image=tf.image.random_brightness(X,max_delta=0.05)  \n",
        "  ds_train=ds_train.map(augment) \n",
        "  for epoch in range(600):\n",
        "   for X,Y in ds_train:\n",
        "     pass\n"
      ],
      "metadata": {
        "id": "S1PZvQ2V4DLF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90d0d3c4-cd41-4acc-8dbe-559bb71be12d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 715 files belonging to 2 classes.\n",
            "Using 644 files for training.\n",
            "Found 715 files belonging to 2 classes.\n",
            "Using 71 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardize data"
      ],
      "metadata": {
        "id": "LcQyadpliYEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "\n",
        "normalized_ds = ds_train.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "# image_batch, labels_batch = next(iter(normalized_ds))\n",
        "# first_image = image_batch[0]\n",
        "# # Notice the pixel values are now in `[0,1]`.\n",
        "# print(np.min(first_image), np.max(first_image))"
      ],
      "metadata": {
        "id": "7svG7aVEidT6"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "ds_train = ds_train.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "ds_validation = ds_validation.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "OOfVjw7_i5nm"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=[\n",
        "        keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    ],\n",
        "    metrics=[\"accuracy\"],\n",
        ")  \n",
        "history=model.fit(ds_train,epochs=10,verbose=2)   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_mpwEt2hz2f",
        "outputId": "56122493-f124-4407-f06c-d1d98d22dd7e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "322/322 - 69s - loss: 3.7915 - accuracy: 0.8758 - 69s/epoch - 214ms/step\n",
            "Epoch 2/10\n",
            "322/322 - 2s - loss: 1.5038 - accuracy: 0.9363 - 2s/epoch - 5ms/step\n",
            "Epoch 3/10\n",
            "322/322 - 2s - loss: 1.1752 - accuracy: 0.9472 - 2s/epoch - 5ms/step\n",
            "Epoch 4/10\n",
            "322/322 - 2s - loss: 0.7501 - accuracy: 0.9658 - 2s/epoch - 5ms/step\n",
            "Epoch 5/10\n",
            "322/322 - 2s - loss: 0.1335 - accuracy: 0.9922 - 2s/epoch - 5ms/step\n",
            "Epoch 6/10\n",
            "322/322 - 2s - loss: 0.1372 - accuracy: 0.9938 - 2s/epoch - 5ms/step\n",
            "Epoch 7/10\n",
            "322/322 - 2s - loss: 0.6830 - accuracy: 0.9736 - 2s/epoch - 5ms/step\n",
            "Epoch 8/10\n",
            "322/322 - 2s - loss: 1.1770 - accuracy: 0.9627 - 2s/epoch - 5ms/step\n",
            "Epoch 9/10\n",
            "322/322 - 2s - loss: 1.8832 - accuracy: 0.9612 - 2s/epoch - 5ms/step\n",
            "Epoch 10/10\n",
            "322/322 - 2s - loss: 0.4693 - accuracy: 0.9814 - 2s/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe97fca63d0>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2tk47z6KhcuO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}